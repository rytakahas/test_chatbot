{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Chatbot with Retrieval-Augmented Generation (RAG)\n",
    "#### This notebook demonstrates how to set up and use the chatbot package, integrating the `microsoft/Phi-3-mini-4k-instruct` model with LangChain and Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "#### Requirements\n",
    "#### To start, you'll need to install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers langchain sentence-transformers faiss-cpu datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.index = faiss.IndexFlatL2(self.embedder.get_sentence_embedding_dimension())\n",
    "        self.documents = []\n",
    "\n",
    "    def add_documents(self, docs):\n",
    "        embeddings = self.embedder.encode(docs)\n",
    "        self.index.add(embeddings)\n",
    "        self.documents.extend(docs)\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        query_embedding = self.embedder.encode([query])\n",
    "        distances, indices = self.index.search(query_embedding, top_k)\n",
    "        return [self.documents[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, model, tokenizer, retriever):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        retrieved_docs = self.retriever.retrieve(query)\n",
    "        context = \" \".join(retrieved_docs)\n",
    "        inputs = self.tokenizer.encode(context + query, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(inputs, max_length=200)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever and add documents\n",
    "retriever = CustomRetriever()\n",
    "retriever.add_documents([\"Document 1: AI advancements in healthcare\", \n",
    "                         \"Document 2: Generative models in NLP\", \n",
    "                         \"Document 3: The future of machine learning\"])\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_pipeline = RAGPipeline(model, tokenizer, retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "def fine_tune_model(model, dataset_path, tokenizer):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)[\"train\"]\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = tokenizer(examples[\"input\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "        labels = tokenizer(examples[\"output\"], truncation=True, padding=\"max_length\", max_length=256).input_ids\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    training_args = TrainingArguments(output_dir=\"./results\", per_device_train_batch_size=4, num_train_epochs=3)\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
    "    trainer.train()\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
